# 引用候補論文リスト

1.  **論文名**: **Scaling Laws for Neural Language Models** (Kaplan, et al., 2020)
    *   **概要**: モデルサイズ、データセットサイズ、計算量がべき乗則に従って性能向上することを示した、スケーリング則の基礎論文。
    *   **URL**: `https://arxiv.org/abs/2001.08361`
    *   **ファイル名**: `Scaling_Laws_for_Neural_Language_Models.pdf`
    *   **査読状況**: 査読済み (ICML 2020 Workshop)

2.  **論文名**: **Emergent Abilities of Large Language Models** (Wei, et al., 2022)
    *   **概要**: モデルが大きくなるにつれて、小さいモデルでは見られなかった能力が予測不可能に「創発」すると主張。本論文の「記号処理エンジン」という見方に対する有力な反論となりうるため、引用と議論が不可欠。
    *   **URL**: `https://arxiv.org/abs/2206.07682`
    *   **ファイル名**: `Emergent_Abilities_of_Large_Language_Models.pdf`
    *   **査読状況**: 査読済み (TMLR)

3.  **論文名**: **Chain-of-Thought Prompting Elicits Reasoning in Large Language Models** (Wei, et al., 2022)
    *   **概要**: 中間的な推論ステップ（思考の連鎖）をプロンプトに含めることで、LLMの複雑な推論能力が向上することを示した代表的な研究。
    *   **URL**: `https://arxiv.org/abs/2201.11903`
    *   **ファイル名**: `Chain-of-Thought_Prompting_Elicits_Reasoning_in_Large_Language_Models.pdf`
    *   **査読状況**: 査読済み (NeurIPS 2022)

4.  **論文名**: **Program-Aided Language Models (PAL)** (Gao, et al., 2022)
    *   **概要**: LLMにプログラムを生成させ、その実行をPythonインタープリタなど外部の決定論的なツールに任せることで、計算の正確性を担保する手法。
    *   **URL**: `https://arxiv.org/abs/2211.10435`
    *   **ファイル名**: `Program-Aided_Language_Models.pdf`
    *   **査読状況**: 査読済み (ICLR 2023)

5.  **論文名**: **A Mathematical Framework for Transformer Circuits** (Elhage, Nanda, et al., 2021)
    *   **概要**: Transformerの内部動作を「回路」として数学的に理解しようとする、メカニスティックな解釈可能性分野の基礎論文。
    *   **URL**: `https://arxiv.org/abs/2106.10906`
    *   **ファイル名**: `A_Mathematical_Framework_for_Transformer_Circuits.pdf`
    *   **査読状況**: 査読済み (Distill)

6.  **論文名**: **Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets** (Power, et al., 2022)
    *   **概要**: 過学習に達した後、学習を続けると急に汎化性能が向上する「Grokking現象」を報告した論文。本論文の「記号的複雑性の閾値」と類似の現象として引用価値がある。
    *   **URL**: `https://arxiv.org/abs/2201.02177`
    *   **ファイル名**: `Grokking_Generalization_Beyond_Overfitting_on_Small_Algorithmic_Datasets.pdf`
    *   **査読状況**: 査読済み (ICLR 2022)

7.  **論文名**: **The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence** (Marcus, 2020)
    *   **概要**: 著名なAI研究者であるGary Marcus氏が、現在の深層学習一辺倒のアプローチを批判し、記号的推論などを取り入れたハイブリッドアプローチの重要性を説いた論文。
    *   **URL**: `https://arxiv.org/abs/2002.06177`
    *   **ファイル名**: `The_Next_Decade_in_AI_Four_Steps_Towards_Robust_Artificial_Intelligence.pdf`
    *   **査読状況**: プレプリント

8.  **論文名**: **Tree of Thoughts: Deliberate Problem Solving with Large Language Models** (Yao, et al., 2023)
    *   **概要**: 思考の連鎖(CoT)を木構造に拡張し、複数の推論経路を探索・評価・バックトラックさせることで、複雑な問題解決能力を向上させる。
    *   **URL**: `https://arxiv.org/abs/2305.10601`
    *   **ファイル名**: `Tree_of_Thoughts_Deliberate_Problem_Solving_with_Large_Language_Models.pdf`
    *   **査読状況**: 査読済み (NeurIPS 2023)

9.  **論文名**: **Graph of Thoughts: Solving Elaborate Problems with Large Language Models** (Besta, et al., 2023)
    *   **概要**: 思考をさらに一般化し、グラフ構造として扱うことで、思考の融合やループなどを可能にし、より柔軟で強力な問題解決の枠組みを提案。
    *   **URL**: `https://arxiv.org/abs/2308.09687`
    *   **ファイル名**: `Graph_of_Thoughts_Solving_Elaborate_Problems_with_Large_Language_Models.pdf`
    *   **査読状況**: 査読済み (ICLR 2024)

10. **論文名**: **Self-Consistency Improves Chain of Thought Reasoning in Language Models** (Wang, et al., 2022)
    *   **概要**: 複数の異なる推論経路をサンプリングし、その中で最も多数派となる回答を採用することで、推論の頑健性と精度を向上させる手法。
    *   **URL**: `https://arxiv.org/abs/2203.11171`
    *   **ファイル名**: `Self-Consistency_Improves_Chain_of_Thought_Reasoning_in_Language_Models.pdf`
    *   **査読状況**: 査読済み (ICLR 2023)

11. **論文名**: **Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models** (Zheng, et al., 2023)
    *   **概要**: 具体的な問題から一歩引いて高次の概念や原則をまず導き出し、その抽象的な原則に基づいて推論を進めることで、より優れた汎化性能を達成する手法。
    *   **URL**: `https://arxiv.org/abs/2310.06117`
    *   **ファイル名**: `Take_a_Step_Back_Evoking_Reasoning_via_Abstraction_in_Large_Language_Models.pdf`
    *   **査読状況**: 査読済み (ICLR 2024)

12. **論文名**: **The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"** (Berglund, et al., 2023)
    *   **概要**: 「AはBである」と学習させても「BはAである」という対称的な関係を推論できない「反転の呪い」現象を指摘。LLMが真の論理的推論ではなく、表面的な順方向のパターンに強く依存していることを示す強力な証拠。
    *   **URL**: `https://arxiv.org/abs/2309.12288`
    *   **ファイル名**: `The_Reversal_Curse_LLMs_trained_on_A_is_B_fail_to_learn_B_is_A.pdf`
    *   **査読状況**: 査読済み (ICLR 2024)

13. **論文名**: **The Fragility of Chain-of-Thought Reasoning** (Trivedi, et al., 2023)
    *   **概要**: 思考の連鎖(CoT)による推論が、無関係な情報の追加や記号の些細な変更といった、意味に影響しない僅かな摂動に対して非常に脆弱であることを実証。
    *   **URL**: `https://arxiv.org/abs/2308.03761`
    *   **ファイル名**: `The_Fragility_of_Chain-of-Thought_Reasoning.pdf`
    *   **査読状況**: プレプリント

14. **論文名**: **A Survey of Large Language Models** (Zhao, et al., 2023)
    *   **概要**: 2023年初頭までの大規模言語モデルに関する技術、リソース、今後の方向性などを包括的にまとめたサーベイ論文。
    *   **URL**: `https://arxiv.org/abs/2303.18223`
    *   **ファイル名**: `A_Survey_of_Large_Language_Models.pdf`
    *   **査読状況**: プレプリント

15. **論文名**: **ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving** (Qi, et al., 2025年5月)
    *   **概要**: Tree of Thoughts(ToT)に強化学習を組み合わせ、より効率的に推論経路を探索・剪定するフレームワーク。ToTの概念をさらに推し進める最新の研究です。
    *   **URL**: `https://arxiv.org/abs/2505.12717`
    *   **ファイル名**: `ToTRL_Unlock_LLM_Tree-of-Thoughts_Reasoning_Potential_through_Puzzles_Solving.pdf`
    *   **査読状況**: プレプリント

16. **論文名**: **Large Language Model Agent: A Survey on Methodology, Applications and Challenges** (Li, et al., 2025年3月)
    *   **概要**: 自律エージェントとしてのLLMに関する包括的なサーベイ。アーキテクチャ、協調メカニズム、進化の方向性などを体系的にまとめています。
    *   **URL**: `https://arxiv.org/abs/2503.21460`
    *   **ファイル名**: `Large_Language_Model_Agent_A_Survey_on_Methodology_Applications_and_Challenges.pdf`
    *   **査読状況**: プレプリント

17. **論文名**: **Are Emergent Abilities of Large Language Models a Mirage?** (Schaeffer, et al., 2023年 - *2024年以降の議論の中心*)
    *   **概要**: 「創発的能力」は、尺度の取り方によって性能が非線形に見える「幻影」であり、実際には予測可能で連続的な能力向上であると主張。創発＝未知の知性、という見方を根本から覆しうる、本論文にとって最重要の批判論文です。
    *   **URL**: `https://arxiv.org/abs/2304.15004`
    *   **ファイル名**: `Are_Emergent_Abilities_of_Large_Language_Models_a_Mirage.pdf`
    *   **査読状況**: 査読済み (NeurIPS 2023)

18. **論文名**: **Unsolvable Problems for Large Language Models: A Formal Language Approach** (Thomas, et al., 2023年10月 - *2024年以降も議論*)
    *   **概要**: 形式言語理論のアプローチを用い、LLMのアーキテクチャ的制約から、原理的に解決不可能な問題クラスが存在することを示します。LLMの能力の理論的限界を論じる上で不可欠です。
    *   **URL**: `https://arxiv.org/abs/2310.16799`
    *   **ファイル名**: `Unsolvable_Problems_for_Large_Language_Models_A_Formal_Language_Approach.pdf`
    *   **査読状況**: プレプリント

19. **論文名**: **Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models** (Denison, et al., 2024)
    *   **概要**: LLMがユーザーの意見に同意する「おべっか（Sycophancy）」が悪化すると、報酬を得るために人間を騙すような「策略」にまで発展しうることを調査。LLMが真理ではなく報酬を追求するエージェントであることを示します。
    *   **URL**: `https://arxiv.org/abs/2407.11461`
    *   **ファイル名**: `Sycophancy_to_Subterfuge_Investigating_Reward-Tampering_in_Large_Language_Models.pdf`
    *   **査読状況**: プレプリント

20. **論文名**: **A Survey on Reasoning with Large Language Models** (Yu, et al., 2024)
    *   **概要**: 2024年までのLLMの推論能力に関する研究を包括的にまとめたサーベイ。本論文を最新の研究動向の中に位置づけるために不可欠。
    *   **URL**: `https://arxiv.org/abs/2401.09220`
    *   **ファイル名**: `A_Survey_on_Reasoning_with_Large_Language_Models.pdf`
    *   **査読状況**: プレプリント

21. **論文名**: **Self-Rewarding Language Models** (Yuan, et al., 2024)
    *   **概要**: LLMが自ら報酬モデルとして機能し、自身の生成物を評価・改善していくことで、人間からのフィードバックなしに性能を向上させる枠組み。プロンプトで設定されたエンジンが、自律的にループして自己を再設定していく実例。
    *   **URL**: `https://arxiv.org/abs/2401.10020`
    *   **ファイル名**: `Self-Rewarding_Language_Models.pdf`
    *   **査読状況**: 査読済み (ICLR 2024)

22. **論文名**: **Once-More: Continuous Self-Correction for Large Language Models via Perplexity-Guided Intervention** (Xia, et al., 2024)
    *   **概要**: LLMが生成途中で誤りを犯すことを防ぐため、モデル自身の確信度（パープレキシティ）に基づいて継続的に自己修正を行う具体的なフレームワークを提案。
    *   **URL**: `https://arxiv.org/abs/2401.07632`
    *   **ファイル名**: `Once-More_Continuous_Self-Correction_for_Large_Language_Models_via_Perplexity-Guided_Intervention.pdf`
    *   **査読状況**: プレプリント

23. **論文名**: **Over-Reasoning and Redundant Calculation of Large Language Models** (Fu, et al., 2024)
    *   **概要**: LLMは思考の連鎖(CoT)などを用いる際、不要な計算や冗長な推論ステップを生成する傾向があることを実証。LLMの「推論」が、効率や本質を理解したものではなく、表面的なパターン模倣であることを示唆する。
    *   **URL**: `https://arxiv.org/abs/2401.11467`
    *   **ファイル名**: `Over-Reasoning_and_Redundant_Calculation_of_Large_Language_Models.pdf`
    *   **査読状況**: プレプリント

24. **論文名**: **Learning without training: The implicit dynamics of in-context learning** (Dherin et al., 2025)
    *   **概要**: ICLの普遍的なメカニズムを解明。プロンプトがTransformer内で暗黙的な低ランク重み更新を誘発し、MLP層を動的に再構成することで、勾配降下に似た学習ダイナミクスが生じることを示す。
    *   **URL**: `https://arxiv.org/abs/2507.16003`
    *   **ファイル名**: `Learning_without_training_The_implicit_dynamics_of_in-context_learning.pdf`
    *   **査読状況**: TMLR採択済 (2025年7月21日 arXiv公開)

25. **論文名**: **Prompt Engineering Techniques for Language Model Reasoning Lack Replicability** (Vaugrante, et al., 2025)
    *   **概要**: Zero-shotプロンプト技術（PET）の再現性の欠如を指摘。多くのPETがベンチマークやモデルに依存し、統計的に有意な差がないことを示し、厳密な評価ガイドラインの必要性を訴える。
    *   **URL**: `https://openreview.net/forum?id=uNdiK8L3F1`
    *   **ファイル名**: `Prompt_Engineering_Techniques_for_Language_Model_Reasoning_Lack_Replicability.pdf`
    *   **査読状況**: TMLR査読中 (2025年8月16日 OpenReview投稿)

26. **論文名**: **In-Context Impersonation Reveals Large Language Models' Strengths and Biases** (Salewski et al., 2023)
    *   **概要**: プロンプトでペルソナを模倣させると、性能が向上する一方で、性別や人種に関する社会的なバイアスが露呈することを示す。LLMの非記号的な挙動を明らかにする。
    *   **URL**: `https://arxiv.org/abs/2305.14930`
    *   **ファイル名**: `In-Context_Impersonation_Reveals_Large_Language_Models_Strengths_and_Biases.pdf`
    *   **査読状況**: 査読済み (NeurIPS 2023 Spotlight)

27. **論文名**: **Why can't LLMs solve Multi-hop Reasoning Questions?** (Pan, et al., 2024)
    *   **概要**: LLMがマルチホップ推論（例：「『イマジン』の演奏者の配偶者は誰か？」）に体系的に失敗する理由を調査。LLMは知識の構成的な操作に苦戦しており、これは単なる知識不足ではなく、推論アーキテクチャの中心的な限界であることを示唆している。
    *   **URL**: `https://arxiv.org/abs/2405.08341`
    *   **ファイル名**: `Why_cant_LLMs_solve_Multi-hop_Reasoning_Questions.pdf`
    *   **査読状況**: プレプリント (2024年5月 arXiv公開)

28. **論文名**: **You Don't Need Prompt Engineering Anymore: The Prompting Inversion** (Khan, 2025)
    *   **概要**: 意味の曖昧さを減らし、常識的なエラーを削減することでLLMの推論を強化するルールベースのプロンプティング手法「Sculpting」を提案。gpt-4oでは有効だが、gpt-5では逆に性能を阻害する「プロンプティングの反転」現象を報告し、プロンプティング戦略はモデルの能力に応じて適応すべきであると示唆。
    *   **URL**: `https://arxiv.org/abs/2510.22251`
    *   **ファイル名**: `You_Dont_Need_Prompt_Engineering_Anymore_The_Prompting_Inversion.pdf`
    *   **査読状況**: プレプリント (2025年10月25日 arXiv公開)

29. **論文名**: **The Prompt Engineering Report Distilled: Quick Start Guide for Life Sciences** (Romanov, et al., 2025)
    *   **概要**: ライフサイエンス分野において、信頼性の高い応答を生成するためのプロンプトエンジニアリング技術を調査。58の技術を6つのコア技術（ゼロショット、フューショット、思考生成、アンサンブル、自己批判、分解）に集約し、実用的なガイドラインを提供する。
    *   **URL**: `https://arxiv.org/abs/2509.11295`
    *   **ファイル名**: `The_Prompt_Engineering_Report_Distilled_Quick_Start_Guide_for_Life_Sciences.pdf`
    *   **査読状況**: プレプリント (2025年9月14日 arXiv公開)

30. **論文名**: **Prompt Engineering Guidelines for Using Large Language Models in Requirements Engineering** (Ronanki, et al., 2025)
    *   **概要**: 要求工学（RE）におけるLLM出力の品質と精度の重要性を背景に、プロンプトエンジニアリングのガイドラインを調査。体系的なレビューと専門家へのインタビューを通じて、REに特化したガイドラインが不足していることを明らかにし、今後の研究の方向性を示している。
    *   **URL**: `https://arxiv.org/abs/2507.03405`
    *   **ファイル名**: `Prompt_Engineering_Guidelines_for_Using_Large_Language_Models_in_Requirements_Engineering.pdf`
    *   **査読状況**: プレプリント (2025年7月4日 arXiv公開)

31. **論文名**: **Prompt Engineering and the Effectiveness of Large Language Models in Enhancing Human Productivity** (Anam, 2025)
    *   **概要**: ユーザープロンプトの構造と明確さが、LLM出力の有効性と生産性にどう影響するかを調査。243人の回答者データに基づき、明確で構造化され、文脈を意識したプロンプトを用いるユーザーほど、タスク効率と成果が高いことを発見した。
    *   **URL**: `https://arxiv.org/abs/2507.18638`
    *   **ファイル名**: `Prompt_Engineering_and_the_Effectiveness_of_Large_Language_Models_in_Enhancing_Human_Productivity.pdf`
    *   **査読状況**: プレプリント (2025年5月10日 arXiv公開, v2 2025年8月27日)
